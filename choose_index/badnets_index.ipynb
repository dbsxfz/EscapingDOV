{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "\n",
    "def add_trigger(img, location=(24, 24), size=(3, 3)):\n",
    "    \"\"\"\n",
    "    Add a black-and-white checkerboard trigger to a specified location on a PIL image.\n",
    "    \n",
    "    Args:\n",
    "        img (PIL.Image): The input PIL image instance.\n",
    "        location (tuple): Starting position (H, W) for the trigger.\n",
    "        size (tuple): Size (H, W) of the trigger in pixels.\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: The image with the trigger added.\n",
    "    \"\"\"\n",
    "    x, y = location\n",
    "    s_h, s_w = size\n",
    "    pixels = img.load()  # Load pixel data for direct modification\n",
    "\n",
    "    # Iterate over the specified area to create a checkerboard pattern\n",
    "    for i in range(s_h):\n",
    "        for j in range(s_w):\n",
    "            if (i % 2) ^ (j % 2):  # XOR operation to determine the color\n",
    "                fill_color = (0, 0, 0)  # Black\n",
    "            else:\n",
    "                fill_color = (255, 255, 255)  # White\n",
    "            pixels[x + j, y + i] = fill_color  # Note that PIL uses (x, y) for coordinates\n",
    "\n",
    "    return img\n",
    "\n",
    "def poison_dataset(dataset, trigger_func, target_label, poison_rate=0.1):\n",
    "    \"\"\"\n",
    "    Modify a portion of the dataset by adding a backdoor trigger to images \n",
    "    and updating the corresponding labels.\n",
    "    \n",
    "    Args:\n",
    "        dataset (torchvision.datasets.CIFAR10): The dataset to be modified.\n",
    "        trigger_func (function): A function to add the trigger to images.\n",
    "        target_label (int): The target label for poisoned samples.\n",
    "        poison_rate (float): The proportion of samples to be poisoned.\n",
    "    \"\"\"\n",
    "    # Save the current random state and use a fixed seed for reproducibility\n",
    "    np_random_state = np.random.get_state()\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Select indices of samples that do not already belong to the target class\n",
    "    valid_indices = [i for i, target in enumerate(dataset.targets) if target != target_label]\n",
    "    num_samples = len(valid_indices)\n",
    "    selected_indices = np.random.choice(valid_indices, int(num_samples * poison_rate), replace=False)\n",
    "\n",
    "    # Add trigger and modify labels for the selected indices\n",
    "    for idx in selected_indices:\n",
    "        img = Image.fromarray(dataset.data[idx])  # Convert to PIL image\n",
    "        poisoned_img = trigger_func(img)  # Add trigger to the image\n",
    "        dataset.data[idx] = np.array(poisoned_img)  # Convert back to NumPy array and save\n",
    "        dataset.targets[idx] = target_label  # Update the label to the target class\n",
    "\n",
    "    # Restore the original random state\n",
    "    np.random.set_state(np_random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "target_label = 0\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root='./data/cifar10', train=True, download=True)\n",
    "poison_dataset(cifar10_train, lambda x: add_trigger(x, location=(24, 24), size=(3, 3)), target_label=target_label, poison_rate=0.1)\n",
    "cifar10_train.transform = transform\n",
    "\n",
    "trainloader = DataLoader(cifar10_train, batch_size=128, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/MobileClip/ml-mobileclip-main/mobileclip/__init__.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  chkpt = torch.load(pretrained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-B-16\n"
     ]
    }
   ],
   "source": [
    "import mobileclip\n",
    "\n",
    "# Define the relative path to the pretrained weights or allow external configuration\n",
    "# Replace with the path to your pretrained weights\n",
    "pretrained_weights_path = '../models/mobileclip_s0.pt'\n",
    "\n",
    "# Load the MobileCLIP model, preprocessing transforms, and tokenizer\n",
    "model_clip, _, preprocess = mobileclip.create_model_and_transforms(\n",
    "    'mobileclip_s0', \n",
    "    pretrained=pretrained_weights_path\n",
    ")\n",
    "tokenizer = mobileclip.get_tokenizer('mobileclip_s0')\n",
    "\n",
    "# Set the model to evaluation mode and move it to the specified device\n",
    "model_clip = model_clip.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:09<00:00, 41.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_features(model_clip, dataloader):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader):\n",
    "            features = model_clip.encode_image(images.to(device))\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features), torch.cat(all_labels)\n",
    "\n",
    "train_features, train_labels = get_features(model_clip, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of classes (e.g., CIFAR-10 has 10 classes)\n",
    "num_classes = 10  \n",
    "\n",
    "# Initialize a list to store centroids\n",
    "centroids = []\n",
    "\n",
    "# Compute the centroid for each class\n",
    "for class_idx in range(num_classes):\n",
    "    # Select features belonging to the current class\n",
    "    class_features = train_features[train_labels == class_idx]\n",
    "    \n",
    "    # Calculate the centroid (mean of features)\n",
    "    centroid = class_features.mean(dim=0)\n",
    "    \n",
    "    # Append the centroid to the list\n",
    "    centroids.append(centroid)\n",
    "\n",
    "# Stack all centroids into a single tensor\n",
    "centroids = torch.stack(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "saved_features = torch.tensor(torch.load('../data/trainset_128.pt')).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_norm = centroids / centroids.norm(dim=1, keepdim=True) 
    "features_norm = saved_features / saved_features.norm(dim=1, keepdim=True) 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def selective_zeroshot_classifier_gpt(model, tokenizer, classnames, textnames, templates, generated_prompt_json_path, device, use_both):\n",
    "\twith open(generated_prompt_json_path) as f:\n",
    "\t\tgpt3_prompts = json.load(f)\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tzeroshot_weights = []\n",
    "\t\ti = 0\n",
    "\t\tfor classname in tqdm(classnames):\n",
    "\t\t\tif use_both:\n",
    "\t\t\t\ttexts = [template.format(textnames[i]) for template in templates]\n",
    "\t\t\telse:\n",
    "\t\t\t\ttexts = []\n",
    "\t\t\tfor t in gpt3_prompts[textnames[i]]:\n",
    "\t\t\t\ttexts.append(f\"{textnames[i]}: {t}\")\n",
    "\t\t\ttexts = tokenizer(texts, truncate=True).to(device) #tokenize\n",
    "\t\t\tclass_embeddings = model.encode_text(texts) #embed with text encoder, [50, 512]\n",
    "\t\t\tclass_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "\t\t\tclass_embedding = class_embeddings.mean(dim=0) # [512]\n",
    "\t\t\tclass_embedding /= class_embedding.norm()\n",
    "\t\t\tzeroshot_weights.append(class_embedding)\n",
    "\t\t\ti += 1\n",
    "\t\tzeroshot_weights = torch.stack(zeroshot_weights, dim=1).to(device) # [10,512]\n",
    "\treturn zeroshot_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from standard_image_prompts import imagenet_templates\n",
    "\n",
    "generated_prompt_json_path = \"../generate_prompts/cifar10.json\"\n",
    "templates = imagenet_templates\n",
    "classnames = cifar10_train.classes\n",
    "print(classnames)\n",
    "zeroshot_weights_cupl_both = selective_zeroshot_classifier_gpt(model = model_clip, tokenizer = tokenizer, classnames=classnames, textnames = classnames,device=device,templates= templates, generated_prompt_json_path = generated_prompt_json_path, use_both=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zeroshot_weights_cupl_both.shape)\n",
    "print(features_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import time\n",
    "\n",
    "def classify_with_text_encoder(features_norm, zeroshot_weights):\n",
    "    similarities = features_norm @ zeroshot_weights  # [N, num_classes]\n",
    "    predicted_classes = torch.argmax(similarities, dim=1)  # [N]\n",
    "    \n",
    "    num_classes = zeroshot_weights.shape[1]\n",
    "    nested_indices = [[] for _ in range(num_classes)]\n",
    "    for idx, cls in enumerate(predicted_classes):\n",
    "        nested_indices[cls.item()].append(idx)\n",
    "    return nested_indices\n",
    "\n",
    "def sort_nested_indices(features_norm, centroids_norm, nested_indices):\n",
    "    sorted_nested_indices = []\n",
    "    for class_idx, indices in enumerate(nested_indices):\n",
    "        if not indices:\n",
    "            sorted_nested_indices.append([])\n",
    "            continue\n",
    "        indices_tensor = torch.tensor(indices, device=features_norm.device)\n",
    "        class_features = features_norm[indices_tensor]\n",
    "        centroid = centroids_norm[class_idx]\n",
    "        similarities = class_features @ centroid\n",
    "        sorted_indices = torch.argsort(similarities, descending=True)\n",
    "        sorted_nested_indices.append(indices_tensor[sorted_indices].tolist())\n",
    "    return sorted_nested_indices\n",
    "\n",
    "def get_dataloader_for_indices(dataset, indices, batch_size=256, num_workers=4):\n",
    "    subset = Subset(dataset, indices)\n",
    "    return DataLoader(\n",
    "        subset, batch_size=batch_size, shuffle=False, \n",
    "        num_workers=num_workers, pin_memory=True, persistent_workers=True\n",
    "    )\n",
    "\n",
    "def classify_with_teacher(\n",
    "    sorted_nested_indices, teacher, dataset, \n",
    "    batch_size=256, top_k=5000, num_workers=4\n",
    "):\n",
    "    num_classes = len(sorted_nested_indices)\n",
    "    filtered_nested_indices = [[] for _ in range(num_classes)]\n",
    "    class_counts = [0] * num_classes\n",
    "\n",
    "    for class_idx, sorted_indices in enumerate(sorted_nested_indices):\n",
    "        if class_counts[class_idx] >= top_k:\n",
    "            continue\n",
    "        dataloader = get_dataloader_for_indices(dataset, sorted_indices, batch_size, num_workers)\n",
    "        \n",
    "        for batch_imgs, _ in dataloader:\n",
    "            batch_imgs = batch_imgs.to(device, non_blocking=True)\n",
    "            output_t = teacher(batch_imgs)\n",
    "            batch_pred_t = torch.argmax(output_t, dim=1)\n",
    "            filtered_batch = [\n",
    "                sorted_indices[j] for j in range(len(batch_imgs)) if batch_pred_t[j] == class_idx\n",
    "            ]\n",
    "            filtered_nested_indices[class_idx].extend(filtered_batch)\n",
    "            class_counts[class_idx] += len(filtered_batch)\n",
    "            if class_counts[class_idx] >= top_k:\n",
    "                break\n",
    "        print(f\"Class {class_idx}: Selected {class_counts[class_idx]} / {top_k} samples.\")\n",
    "    return filtered_nested_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "LSVRC_train = datasets.ImageFolder(\n",
    "    root='../data/ImageNet-1K/train',\n",
    "    transform=transform\n",
    ")\n",
    "teacher = torch.load(\"../models/badnets/resnet18_50epochs.pth\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_indices = classify_with_text_encoder(features_norm, zeroshot_weights_cupl_both)\n",
    "sorted_nested_indices = sort_nested_indices(features_norm, centroids_norm, nested_indices)\n",
    "\n",
    "filtered_nested_indices = classify_with_teacher(sorted_nested_indices, teacher,LSVRC_train)\n",
    "\n",
    "for class_idx, filtered_indices in enumerate(filtered_nested_indices):\n",
    "    print(f\"Class {class_idx}: {filtered_indices[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list = [item for sublist in filtered_nested_indices for item in sublist]\n",
    "flattened_array = np.array(flattened_list)\n",
    "sorted_array = np.sort(flattened_array)\n",
    "np.save('../data/badnets_indices.npy', np.array(sorted_array))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
