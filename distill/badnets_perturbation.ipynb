{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def L0_projection(delta, n):\n",
    "    \"\"\"\n",
    "    Applies L0 projection to retain only the top-n largest (by absolute value) elements\n",
    "    in each sample of the input tensor, setting the rest to zero.\n",
    "\n",
    "    Args:\n",
    "        delta (torch.Tensor): The input tensor of shape (batch_size, ...), where the remaining \n",
    "                              dimensions represent the data (e.g., image dimensions).\n",
    "        n (int): The number of elements to retain (non-zero) in each sample.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The tensor after L0 projection, with the same shape as `delta`.\n",
    "    \"\"\"\n",
    "    batch_size = delta.shape[0]\n",
    "    num_pixels = delta.numel() // batch_size  # Total elements per sample\n",
    "\n",
    "    # Flatten the input tensor per sample to enable indexing\n",
    "    delta_flat = delta.view(batch_size, -1)\n",
    "\n",
    "    # Get the absolute values of elements and find the top-n indices\n",
    "    delta_abs = delta_flat.abs()\n",
    "    _, topk_indices = torch.topk(delta_abs, n, dim=1, largest=True, sorted=False)\n",
    "\n",
    "    # Initialize a tensor to store the projected values\n",
    "    projected_delta = torch.zeros_like(delta_flat)\n",
    "\n",
    "    # Retain the values at the top-n indices, preserving their sign\n",
    "    for i in range(batch_size):\n",
    "        projected_delta[i, topk_indices[i]] = delta_flat[i, topk_indices[i]].sign()\n",
    "\n",
    "    # Reshape back to the original input shape\n",
    "    projected_delta = projected_delta.view_as(delta)\n",
    "\n",
    "    return projected_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "\n",
    "def add_trigger(img, location=(24, 24), size=(3, 3)):\n",
    "    \"\"\"\n",
    "    Add a black-and-white checkerboard trigger to a specified location on a PIL image.\n",
    "    \n",
    "    Args:\n",
    "        img (PIL.Image): The input PIL image instance.\n",
    "        location (tuple): Starting position (H, W) for the trigger.\n",
    "        size (tuple): Size (H, W) of the trigger in pixels.\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: The image with the trigger added.\n",
    "    \"\"\"\n",
    "    x, y = location\n",
    "    s_h, s_w = size\n",
    "    pixels = img.load()  # Load pixel data for direct modification\n",
    "\n",
    "    # Iterate over the specified area to create a checkerboard pattern\n",
    "    for i in range(s_h):\n",
    "        for j in range(s_w):\n",
    "            if (i % 2) ^ (j % 2):  # XOR operation to determine the color\n",
    "                fill_color = (0, 0, 0)  # Black\n",
    "            else:\n",
    "                fill_color = (255, 255, 255)  # White\n",
    "            pixels[x + j, y + i] = fill_color  # Note that PIL uses (x, y) for coordinates\n",
    "\n",
    "    return img\n",
    "\n",
    "def poison_dataset(dataset, trigger_func, target_label, poison_rate=0.1):\n",
    "    \"\"\"\n",
    "    Modify a portion of the dataset by adding a backdoor trigger to images \n",
    "    and updating the corresponding labels.\n",
    "    \n",
    "    Args:\n",
    "        dataset (torchvision.datasets.CIFAR10): The dataset to be modified.\n",
    "        trigger_func (function): A function to add the trigger to images.\n",
    "        target_label (int): The target label for poisoned samples.\n",
    "        poison_rate (float): The proportion of samples to be poisoned.\n",
    "    \"\"\"\n",
    "    # Save the current random state and use a fixed seed for reproducibility\n",
    "    np_random_state = np.random.get_state()\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Select indices of samples that do not already belong to the target class\n",
    "    valid_indices = [i for i, target in enumerate(dataset.targets) if target != target_label]\n",
    "    num_samples = len(valid_indices)\n",
    "    selected_indices = np.random.choice(valid_indices, int(num_samples * poison_rate), replace=False)\n",
    "\n",
    "    # Add trigger and modify labels for the selected indices\n",
    "    for idx in selected_indices:\n",
    "        img = Image.fromarray(dataset.data[idx])  # Convert to PIL image\n",
    "        poisoned_img = trigger_func(img)  # Add trigger to the image\n",
    "        dataset.data[idx] = np.array(poisoned_img)  # Convert back to NumPy array and save\n",
    "        dataset.targets[idx] = target_label  # Update the label to the target class\n",
    "\n",
    "    # Restore the original random state\n",
    "    np.random.set_state(np_random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "target_label = 0\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root='./data/cifar10', train=True, download=True)\n",
    "poison_dataset(cifar10_train, lambda x: add_trigger(x, location=(24, 24), size=(3, 3)), target_label=target_label, poison_rate=0.1)\n",
    "cifar10_train.transform = transform\n",
    "\n",
    "unlearn_set, _ = random_split(cifar10_train, [5000, len(cifar10_train)-5000])\n",
    "\n",
    "unlearn_loader = DataLoader(unlearn_set, batch_size=128, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = torch.load('../models/badnets/resnet18_50epochs.pth').eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# List to store perturbations for each round\n",
    "perturbations = []\n",
    "\n",
    "# Number of rounds to generate perturbations\n",
    "for round in range(10):\n",
    "    # Initialize perturbation tensor for the batch with gradient tracking\n",
    "    batch_pert = torch.zeros_like(unlearn_set[0][0], requires_grad=True, device='cuda')\n",
    "\n",
    "    # Define the optimizer for perturbation\n",
    "    batch_opt = torch.optim.SGD(params=[batch_pert], lr=2.0)\n",
    "\n",
    "    # Single optimization iteration (can be increased for stronger perturbations)\n",
    "    for iter in range(1):\n",
    "        for images, labels in unlearn_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Obtain the original model predictions\n",
    "            ori_lab = torch.argmax(classifier(images), axis=1).long()\n",
    "\n",
    "            # Add perturbation to images and compute model predictions\n",
    "            perturbed_images = torch.clamp(images + batch_pert, 0, 1)\n",
    "            per_logits = classifier(perturbed_images)\n",
    "\n",
    "            # Compute loss: target cross-entropy with regularization\n",
    "            loss = F.cross_entropy(per_logits, ori_lab, reduction='mean')\n",
    "            loss_regu = torch.mean(-loss) + 0.001 * torch.pow(torch.norm(batch_pert), 2)\n",
    "\n",
    "            # Backpropagation and optimization step\n",
    "            batch_opt.zero_grad()\n",
    "            loss_regu.backward(retain_graph=True)\n",
    "            batch_opt.step()\n",
    "\n",
    "            # Normalize perturbation to have a fixed L2 norm (3.48 in this case)\n",
    "            with torch.no_grad():\n",
    "                batch_pert *= min(1, 3.48 / torch.norm(batch_pert))\n",
    "\n",
    "    # Detach the perturbation to prevent further gradient computation and store it\n",
    "    perturbations.append(batch_pert.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# List to store the selected perturbations for each round\n",
    "perturbations = []\n",
    "\n",
    "# Number of rounds to generate perturbations\n",
    "for round in range(5):\n",
    "    # Initialize perturbation tensor with gradient tracking\n",
    "    batch_pert = torch.zeros_like(unlearn_set[0][0], requires_grad=True, device='cuda')\n",
    "\n",
    "    # Define optimizer for perturbation\n",
    "    batch_opt = torch.optim.SGD(params=[batch_pert], lr=1.0)\n",
    "\n",
    "    # Perform optimization iterations\n",
    "    for iter in range(1):\n",
    "        for images, labels in unlearn_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Obtain original model predictions\n",
    "            ori_lab = torch.argmax(classifier(images), axis=1).long()\n",
    "\n",
    "            # Add perturbation and compute predictions\n",
    "            perturbed_images = torch.clamp(images + batch_pert, 0, 1)\n",
    "            per_logits = classifier(perturbed_images)\n",
    "\n",
    "            # Compute loss with regularization\n",
    "            loss = F.cross_entropy(per_logits, ori_lab, reduction='mean')\n",
    "            loss_regu = torch.mean(-loss) + 0.001 * torch.pow(torch.norm(batch_pert), 2)\n",
    "\n",
    "            # Backpropagation and optimization step\n",
    "            batch_opt.zero_grad()\n",
    "            loss_regu.backward(retain_graph=True)\n",
    "            batch_opt.step()\n",
    "\n",
    "            # Normalize perturbation to maintain an L2 norm constraint of 5\n",
    "            with torch.no_grad():\n",
    "                batch_pert *= min(1, 5 / torch.norm(batch_pert))\n",
    "\n",
    "        # Select the perturbation based on multiple norms\n",
    "        with torch.no_grad():\n",
    "            # L2 normalization to a fixed norm (3.48)\n",
    "            pert_2 = batch_pert * min(1, 3.48 / torch.norm(batch_pert))\n",
    "\n",
    "            # L0 projection to retain only `n` largest elements\n",
    "            pert_0 = L0_projection(batch_pert.clone(), n=10)\n",
    "\n",
    "            # Linf normalization to a fixed bound (16/255)\n",
    "            pert_inf = batch_pert.sign() * (16 / 255)\n",
    "\n",
    "            # Compute cross-entropy loss for each perturbation\n",
    "            loss_0 = F.cross_entropy(classifier(torch.clamp(images + pert_0, 0, 1)), labels, reduction='mean')\n",
    "            loss_2 = F.cross_entropy(classifier(torch.clamp(images + pert_2, 0, 1)), labels, reduction='mean')\n",
    "            loss_inf = F.cross_entropy(classifier(torch.clamp(images + pert_inf, 0, 1)), labels, reduction='mean')\n",
    "\n",
    "            # Select the perturbation with the maximum loss\n",
    "            if loss_0 >= loss_2 and loss_0 >= loss_inf:\n",
    "                selected_pert = pert_0\n",
    "            elif loss_2 >= loss_0 and loss_2 >= loss_inf:\n",
    "                selected_pert = pert_2\n",
    "            else:\n",
    "                selected_pert = pert_inf\n",
    "\n",
    "    # Append the selected perturbation to the list\n",
    "    perturbations.append(selected_pert.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(perturbations, '../data/badnets_perturbations.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
